# Markov Reward Process
### Markov process
- a turple $<\mathcal{S,P}>$


$\mathcal{S}$ set of finite states\
$\mathcal{P}$ state transition martix\
$\mathcal{P}_{\,i,j}=P(s_i\,\,|\; s_j)$ $s_j$转移到$s_i$的概率

### Markov Reward process
- $<\mathcal{S,P},r,\gamma>$ 

> $r$ reward function $r(s)=E[R_t\,|\,S_t=s]$
> $\gamma$ discount factor

$G_t$ 回报
> $\begin{align}G_t&=R_t+\gamma\,R_{t+1}+\gamma^2R_{t+2}+...\\ &=\sum_{k=0}^{\infty}\gamma^kR_{t+k}\end{align}$
> $\gamma\to0$ short term reward weigh
> $\gamma\to1$ long term reward weigh

$V(s)$ value function
> 通过转移概率可以得到

$$
\begin{align}V(s)&=
E[G_t\ |\ S_t=s]\\
&=E[R_t+\gamma\,R_{t+1}+\gamma^2R_{t+2}+...\,|\,S_t=s]\\
&=E[R_t+\gamma G_{t+1}\,|\,S_t=s]\\
\end{align}-----------(1)
$$
Bellman euqation 
$$V(s)=r(s)+\gamma\sum_{s'\in S}{P(s'|s)V(s')} ----(2)$$
V(s)是一个状态的价值。价值即为回报$G_t$的期望。   
一个状态的价值可以表示为进入这一状态时获得的reward  +   其后续状态会带来的回报。具体的计算方式才用的是乘上衰减因子$\gamma$。
贝尔曼方程描述了一种计算V(s)的方法。前半部分进入这一状态时获得的reward与V(s)的定义一样，后半部采用了**递归**的方式计算。$P(s'|s)V(s')$表示了后续状态的奖励的期望。


in martix form:
$$\begin{align}\mathcal{V}&=\mathcal{R}+\gamma\mathcal{PV}\\
(\mathcal{I}-\gamma\mathcal{P})\mathcal{V}&=\mathcal{R}\\
\mathcal{V}&=(\mathcal{I}-\gamma\mathcal{P})^{-1}\mathcal{R}
\end{align}$$
this method is $O(n^3)$
### Markov Decision Process
- $<\mathcal{S,A,P},r,\gamma>$
	$P[s'|s,a]$ state transition function instead of a martix
	$r=r(s,a)=E[R_t|S_t=s,A_t=a]$

# the Agent-environment Interface


# optimal policy and optimal value functions
>  If the dynamics p of the environment are known, then in principle one can solve this system of equations for $v_*$ using any one of a variety of methods for solving systems of nonlinear equations.
[[RLbook2020trimmed.pdf#page=86&selection=242,9,253,80|RLbook2020trimmed, page 86]]

note: dynamics of MDP is defined as func. p， which is
$$p(s',r|s,a)=Pr\{S_t =s', R_t=r|S_{t-1}=s,A_{t-1}=a\}$$
All in all func. p describes the whole transaction state of a MDP$<\mathcal{S,A,P},r,\gamma>$

$$\sqrt{6-\sqrt{6}-2\sqrt{3}}$$