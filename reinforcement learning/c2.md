feeedback
>depend on actions taken ---------evaluative
>independent on actions taken-----instructive------supervised learning

nonassociative setting [[RLbook2020trimmed.pdf#page=47&selection=29,0,31,7|RLbook2020trimmed, page 47]]
>bandit problem (in this chapter)
>only 1 state

## 1 
- time step $t$ 
- action $A_t$ 
- reward $R_t$ 
- value $q_{*}(a)=E[R_t|A_t=a]$
- estimate $Q_t(a)$
balance and conflict
>explore -> greedy
>exploit -> improve estimate

sample-average method
>take mean as estimate

k-arm

$\epsilon-greedy$ method


