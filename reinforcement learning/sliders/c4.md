DP dynamic programming 
MDP Markov Decision Process
compute value functions 
### 4.1 Policy Evaluation
1. get state value function $v_{\pi}$
$$\begin{align}v_{\pi}(s)&=E_{\pi}[R_{t+1}+\gamma G_{t+1}| S_t=s]\\v_{k+1}(s)&=E_{\pi}[R_{t+1}+\gamma v_k(S_{t+1})| S_t=s]\end{align}$$
2. iterative policy evaluation[[RLbook2020trimmed.pdf#page=96&selection=280,1,304,1|RLbook2020trimmed, page 96]]
	expected update
	$lim_{k\to \infty}v_{k}(s)=v_{\pi}$

### 4.2 policy improvement

|s --> | a1  -->  |s'1|
|-----:|-----:|-----|
 | | |--> |  s'2|
 | | -->| a2 -->  |....|
$v_{\pi}(s)$ | $q_{\pi}(s,a)$|$v_{\pi}(s')$
compare $v_{\pi}(s)$ with $q_{\pi}(s,a)$
use $v_{\pi}(s')$ to compute $q_{\pi}(s,a)$
while we get $v_*$ , (optimal) greedy policy $\pi$ make $v_{\pi}(s)$ satisfy bellman optimality equation 

this case is only for deterministic policy. it can be extended to stochastic policy and any apportioining scheme. 

### 4.3 policy improvement
one iteration:
$\pi_0$ evaluate $v_{\pi_0}$ improve $\pi_1$ 


https://zhuanlan.zhihu.com/p/35245722
https://www.jianshu.com/p/d347bb2ca53c
